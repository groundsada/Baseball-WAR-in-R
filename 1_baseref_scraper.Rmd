---
title: "R Notebook"
output: html_notebook
---

# Introduction

This notebook details the process of collecting and analyzing baseball team data using RSelenium to scrape data from the web, and then cleaning and preparing that data for further analysis.

## Setup Libraries

First, we load necessary libraries for web scraping and data handling.

```{r}
library(netstat)
library(RSelenium)
library(wdman)
```

## Initialize Selenium

Initialize RSelenium to start interacting with web browsers automatically.

```{r}
selenium()
selenium_object <- selenium(retcommand = T, check = F)
```

# Scrape Baseball Team Data

Here we setup RSelenium, navigate to a specific baseball team's page, scrape the table data, and then close the Selenium driver.

```{r}
library(RSelenium)

remote_driver <- rsDriver(browser = "chrome", chromever = "113.0.5672.63", verbose = FALSE, port = 2254L)
remDr <- remote_driver$client

remDr$navigate("https://www.baseball-reference.com/teams/ARI/2023.shtml")

tables <- remDr$findElements(using = "css selector", "table")

third_table_element <- tables[[3]]

table_html <- third_table_element$getElementAttribute("outerHTML")[[1]]

library(rvest)
table_df <- read_html(table_html) %>% html_table() %>% .[[1]]

print(table_df)

remDr$close()
remote_driver$server$stop()
```


## Setup for Multiple Teams and Years

Prepare to scrape data across multiple teams and multiple seasons. This includes setting a timeout to handle page loads and defining directories for storing the data.

```{r}
library(RSelenium)
library(rvest)
library(dplyr)

teams <- c("ARI", "ATL", "BAL", "BOS", "CHC", "CHW", "CIN", "CLE", "COL", "DET", "HOU", "KCR", "LAA", "LAD", "MIA", "MIL", "MIN", "NYM", "NYY", "OAK", "PHI", "PIT", "SDP", "SEA", "SFG", "STL", "TBR", "TEX", "TOR", "WSN")
years <- 2003:2023

remote_driver <- rsDriver(browser = "chrome", chromever = "113.0.5672.63", verbose = FALSE, port = 4454L)
remDr <- remote_driver$client

```

## Data Download Function

Function to download and save data for each team and year. Handles data cleaning and checks for existing files to avoid unnecessary downloads.


```{r}
remDr$setTimeout(type = "page load", milliseconds = 170000)

clean_names <- function(df) {
  names(df) <- make.names(names(df), unique = TRUE)
  return(df)
}


dir.create("data1", showWarnings = FALSE)
dir.create("data2", showWarnings = FALSE)

team_mapping <- list(
  LAA = "ANA",
  TBR = "TBD",
  WSN = "MON"
)

```


```{r}
download_data <- function(team, year) {
  correct_team <- ifelse(team %in% names(team_mapping) && year < 2008, team_mapping[[team]], team)
  
  file_path1 <- paste0("data1/", team, "_", year, "_table1.csv")
  file_path3 <- paste0("data2/", team, "_", year, "_table3.csv")
  
  if (!file.exists(file_path1) || !file.exists(file_path3)) {
    url <- paste0("https://www.baseball-reference.com/teams/", correct_team, "/", year, ".shtml")
    remDr$navigate(url)
    
    tables <- remDr$findElements(using = "css selector", "table")
    
    if (!file.exists(file_path1) && length(tables) >= 1) {
      first_table_element <- tables[[1]]
      first_table_html <- first_table_element$getElementAttribute("outerHTML")[[1]]
      first_table_df <- read_html(first_table_html) %>% html_table() %>% .[[1]] %>% clean_names()
      write.csv(first_table_df, file = file_path1, row.names = FALSE)
    }
    if (!file.exists(file_path3) && length(tables) >= 3) {
      third_table_element <- tables[[3]]
      third_table_html <- third_table_element$getElementAttribute("outerHTML")[[1]]
      third_table_df <- read_html(third_table_html) %>% html_table() %>% .[[1]] %>% clean_names()
      write.csv(third_table_df, file = file_path3, row.names = FALSE)
    }
  }
}
```


## Execute Data Download

Iteratively download data for all teams and years specified. Handles errors and retries failed downloads.


```{r}
for (team in teams) {
  for (year in years) {
    while (TRUE) {
      tryCatch({
        download_data(team, year)
        break
      }, error = function(e) {
        cat("Error for", team, year, ":", e$message, "- retrying...\n")
      })
    }
  }
}

remDr$close()
remote_driver$server$stop()

```

## Verify Downloaded Data

Checks for missing files in the dataset and lists them if any are found.


```{r}
expected_files_data1 <- sapply(years, function(year) {
  paste0("data1/", teams, "_", year, "_table1.csv")
}, simplify = FALSE) %>% unlist()

expected_files_data2 <- sapply(years, function(year) {
  paste0("data2/", teams, "_", year, "_table3.csv")
}, simplify = FALSE) %>% unlist()

expected_files <- c(expected_files_data1, expected_files_data2)

actual_files_data1 <- list.files("data1", full.names = TRUE)
actual_files_data2 <- list.files("data2", full.names = TRUE)
actual_files <- c(actual_files_data1, actual_files_data2)

missing_files <- setdiff(expected_files, actual_files)

if (length(missing_files) > 0) {
  cat("Missing files:\n")
  print(missing_files)
} else {
  cat("All files have been downloaded.\n")
}

```

## Merge and Prepare Final Dataset

Merges all CSV files into a single dataset, cleans up the data, and prepares it for analysis.

```{r}
dir.create("dataset", showWarnings = FALSE)

merge_csv_files <- function(input_folder, output_file) {
  all_files <- list.files(input_folder, full.names = TRUE, pattern = "\\.csv$")
  all_data <- lapply(all_files, function(file) {
    data <- read.csv(file)
    file_parts <- strsplit(basename(file), "_")[[1]]
    data$Team <- file_parts[1]
    data$Season <- as.numeric(file_parts[2])
    return(data)
  })
  merged_data <- bind_rows(all_data) %>%
    arrange(Team, Season, Name)
  write.csv(merged_data, file = output_file, row.names = FALSE)
}
```

```{r}
merge_csv_files("data1", "dataset/data1.csv")
merge_csv_files("data2", "dataset/data2.csv")
```

# Final Notes

This notebook provides a comprehensive guide to downloading and preparing data for baseball team analysis. Note that team names may have changed over the years, and name duplicates might occur when merging rank data.


