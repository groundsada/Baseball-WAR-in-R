print(paste("Percentage of total variance explained by PC1 and PC2 are", variance_explained[1], "% and", variance_explained[2], "%, respectively."))
data("USArrests")
arrest_data <- na.omit(USArrests)
head(arrest_data)
summary(arrest_data)
print("Data is scaled due to different units and scales of features.")
arrest_data_scaled <- scale(arrest_data)
wss <- numeric(9)
for (k in 2:10) {
kmeans_model <- kmeans(arrest_data_scaled, centers = k, nstart = 25)
wss[k - 1] <- kmeans_model$tot.withinss
}
wss
print("Plotting within-cluster sum of squares for each k")
wss_df <- tibble(K = 2:10, WSS = wss)
ggplot(wss_df, aes(x = K, y = WSS)) +
geom_line() +
geom_point() +
labs(title = "Elbow Method for Choosing k", x = "Number of Clusters (k)", y = "Within-cluster Sum of Squares")
optimal_k <- which(diff(diff(wss)) == min(diff(diff(wss)))) + 2
print(paste("The optimal number of clusters is", optimal_k))
final_kmeans <- kmeans(arrest_data_scaled, centers = optimal_k, nstart = 25)
print(final_kmeans)
print("Plotting optimal clustering")
final_kmeans <- kmeans(arrest_data_scaled, centers = 8, nstart = 25)
fviz_cluster(final_kmeans, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
print("Plotting clustering for k = 4")
final_kmeans_k4 <- kmeans(arrest_data_scaled, centers = 4, nstart = 25)
fviz_cluster(final_kmeans_k4, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine_quality <- read.csv(url, sep = ";")
wine_data <- wine_quality[, -ncol(wine_quality)]
summary(wine_data)
wine_data_scaled <- scale(wine_data)
summary(wine_data_scaled)
hc_single <- hclust(dist(wine_data_scaled), method = "single")
hc_complete <- hclust(dist(wine_data_scaled), method = "complete")
plot(hc_single, main = "Single Linkage", sub = "", xlab = "")
plot(hc_complete, main = "Complete Linkage", sub = "", xlab = "")
distance_single <- hc_single$height[length(hc_single$height)-1]
distance_complete <- hc_complete$height[length(hc_complete$height)-1]
print(paste("Distance at which penultimate clusters merge for single linkage:", distance_single))
print(paste("Distance at which penultimate clusters merge for complete linkage:", distance_complete))
clusters_single <- cutree(hc_single, k = 2)
clusters_complete <- cutree(hc_complete, k = 2)
summary_stats_single <- aggregate(wine_data, by = list(cluster = clusters_single), mean)
summary_stats_complete <- aggregate(wine_data, by = list(cluster = clusters_complete), mean)
summary_stats_single
summary_stats_complete
mean_diffs_single <- sapply(2:ncol(summary_stats_single), function(i) {
abs(summary_stats_single[[i]][1] - summary_stats_single[[i]][2])
})
names(mean_diffs_single) <- colnames(wine_data)
mean_diffs_complete <- sapply(2:ncol(summary_stats_complete), function(i) {
abs(summary_stats_complete[[i]][1] - summary_stats_complete[[i]][2])
})
names(mean_diffs_complete) <- colnames(wine_data)
print("Feature means with the largest differences for Single Linkage")
print(mean_diffs_single)
print("Feature means with the largest differences for Complete Linkage")
print(mean_diffs_complete)
sorted_diffs_single <- sort(mean_diffs_single, decreasing = TRUE)
top_3_single <- sorted_diffs_single[1:3]
print("Feature means with the largest differences for Single Linkage:")
print(top_3_single)
sorted_diffs_complete <- sort(mean_diffs_complete, decreasing = TRUE)
top_3_complete <- sorted_diffs_complete[1:3]
print("Feature means with the largest differences for Complete Linkage:")
print(top_3_complete)
sizes_single <- table(clusters_single)
sizes_complete <- table(clusters_complete)
balance_single <- abs(sizes_single[1] - sizes_single[2])
balance_complete <- abs(sizes_complete[1] - sizes_complete[2])
print(paste("Balance of clustering for single linkage:", balance_single))
print(paste("Balance of clustering for complete linkage:", balance_complete))
sizes_single
sizes_complete
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)
# surpress warnings
options(warn = -1)
# install libraries (if not installed) and load them
packages <- c("utils", "ggplot2", "tidyverse", "factoextra", "cluster")
for (pkg in packages) {
suppressWarnings(
suppressMessages(
if (!require(pkg, character.only = TRUE)) {
install.packages(pkg)
library(pkg, character.only = TRUE)
}
)
)
}
m <- matrix(c(0, 0.3, 0.4, 0.7, 0.3, 0, 0.5, 0.8, 0.4, 0.5, 0., 0.45, 0.7, 0.8, 0.45, 0), ncol = 4)
c1 <- hclust(as.dist(m), method = "complete")
plot(c1)
c2 <- hclust(as.dist(m), method = "single")
plot(c2)
table(1:4, cutree(c1, 2))
table(1:4, cutree(c2, 2))
plot(c1, labels = c(2, 1, 3, 4))
library(ggplot2)
d <- data.frame(
x1 = c(1, 1, 0, 5, 6, 4),
x2 = c(4, 3, 4, 1, 2, 0)
)
ggplot(d, aes(x = x1, y = x2)) + geom_point()
set.seed(42)
d$cluster <- sample(c(1, 2), size = nrow(d), replace = TRUE)
d
centroids <- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2]))
centroids
dist <- sapply(1:2, function(i) {
sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2)
})
d$cluster <- apply(dist, 1, which.min)
d
centroids <- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2]))
dist <- sapply(1:2, function(i) {
sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2)
})
d$cluster <- apply(dist, 1, which.min)
d
ggplot(d, aes(x = x1, y = x2, color = factor(cluster))) + geom_point()
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine_data <- read.csv(url, header = FALSE)
colnames(wine_data) <- c("Class", "Alcohol", "Malic_Acid", "Ash", "Alcalinity_of_Ash", "Magnesium", "Total_phenols", "Flavanoids", "Nonflavanoid_phenols", "Proanthocyanins", "Color_intensity", "Hue", "OD280_OD315", "Proline")
head(wine_data)
summary(wine_data)
print("Performing PCA with scaling")
wine_pca <- prcomp(wine_data[, -1], scale = TRUE)
print("Plotting biplot")
biplot(wine_pca, cex=0.5)
cor(wine_data[, "Hue"], wine_data[,"Malic_Acid"])
print("Identifying feature opposite to Hue")
biplot_vectors <- wine_pca$rotation[, 1:2]
hue_vector <- biplot_vectors["Hue",]
opposite_feature <- which.min(colSums(t(biplot_vectors) * hue_vector))
print(opposite_feature)
correlation_value <- cor(wine_data$Hue, wine_data[, opposite_feature + 1])
print(paste("The feature", colnames(wine_data)[opposite_feature + 1], "has a correlation of", correlation_value, "with Hue"))
print("Plotting scree plot")
screeplot(wine_pca)
summary(wine_pca)
variance_explained <- summary(wine_pca)$importance[2, 1:2] * 100
print(paste("Percentage of total variance explained by PC1 and PC2 are", variance_explained[1], "% and", variance_explained[2], "%, respectively."))
data("USArrests")
arrest_data <- na.omit(USArrests)
head(arrest_data)
summary(arrest_data)
print("Data is scaled due to different units and scales of features.")
arrest_data_scaled <- scale(arrest_data)
wss <- numeric(9)
for (k in 2:10) {
kmeans_model <- kmeans(arrest_data_scaled, centers = k, nstart = 25)
wss[k - 1] <- kmeans_model$tot.withinss
}
wss
print("Plotting within-cluster sum of squares for each k")
wss_df <- tibble(K = 2:10, WSS = wss)
ggplot(wss_df, aes(x = K, y = WSS)) +
geom_line() +
geom_point() +
labs(title = "Elbow Method for Choosing k", x = "Number of Clusters (k)", y = "Within-cluster Sum of Squares")
optimal_k <- which(diff(diff(wss)) == min(diff(diff(wss)))) + 2
print(paste("The optimal number of clusters is", optimal_k))
final_kmeans <- kmeans(arrest_data_scaled, centers = optimal_k, nstart = 25)
print(final_kmeans)
print("Plotting optimal clustering")
final_kmeans <- kmeans(arrest_data_scaled, centers = 8, nstart = 25)
fviz_cluster(final_kmeans, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
print("Plotting clustering for k = 4")
final_kmeans_k4 <- kmeans(arrest_data_scaled, centers = 4, nstart = 25)
fviz_cluster(final_kmeans_k4, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine_quality <- read.csv(url, sep = ";")
wine_data <- wine_quality[, -ncol(wine_quality)]
summary(wine_data)
wine_data_scaled <- scale(wine_data)
summary(wine_data_scaled)
hc_single <- hclust(dist(wine_data_scaled), method = "single")
hc_complete <- hclust(dist(wine_data_scaled), method = "complete")
plot(hc_single, main = "Single Linkage", sub = "", xlab = "")
plot(hc_complete, main = "Complete Linkage", sub = "", xlab = "")
distance_single <- hc_single$height[length(hc_single$height)-1]
distance_complete <- hc_complete$height[length(hc_complete$height)-1]
print(paste("Distance at which penultimate clusters merge for single linkage:", distance_single))
print(paste("Distance at which penultimate clusters merge for complete linkage:", distance_complete))
clusters_single <- cutree(hc_single, k = 2)
clusters_complete <- cutree(hc_complete, k = 2)
summary_stats_single <- aggregate(wine_data, by = list(cluster = clusters_single), mean)
summary_stats_complete <- aggregate(wine_data, by = list(cluster = clusters_complete), mean)
summary_stats_single
summary_stats_complete
mean_diffs_single <- sapply(2:ncol(summary_stats_single), function(i) {
abs(summary_stats_single[[i]][1] - summary_stats_single[[i]][2])
})
names(mean_diffs_single) <- colnames(wine_data)
mean_diffs_complete <- sapply(2:ncol(summary_stats_complete), function(i) {
abs(summary_stats_complete[[i]][1] - summary_stats_complete[[i]][2])
})
names(mean_diffs_complete) <- colnames(wine_data)
print("Feature means with the largest differences for Single Linkage")
print(mean_diffs_single)
print("Feature means with the largest differences for Complete Linkage")
print(mean_diffs_complete)
sorted_diffs_single <- sort(mean_diffs_single, decreasing = TRUE)
top_3_single <- sorted_diffs_single[1:3]
print("Feature means with the largest differences for Single Linkage:")
print(top_3_single)
sorted_diffs_complete <- sort(mean_diffs_complete, decreasing = TRUE)
top_3_complete <- sorted_diffs_complete[1:3]
print("Feature means with the largest differences for Complete Linkage:")
print(top_3_complete)
sizes_single <- table(clusters_single)
sizes_complete <- table(clusters_complete)
balance_single <- abs(sizes_single[1] - sizes_single[2])
balance_complete <- abs(sizes_complete[1] - sizes_complete[2])
print(paste("Balance of clustering for single linkage:", balance_single))
print(paste("Balance of clustering for complete linkage:", balance_complete))
sizes_single
sizes_complete
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)
# surpress warnings
options(warn = -1)
# install libraries (if not installed) and load them
packages <- c("utils", "ggplot2", "tidyverse", "factoextra", "cluster")
for (pkg in packages) {
suppressWarnings(
suppressMessages(
if (!require(pkg, character.only = TRUE)) {
install.packages(pkg)
library(pkg, character.only = TRUE)
}
)
)
}
m <- matrix(c(0, 0.3, 0.4, 0.7, 0.3, 0, 0.5, 0.8, 0.4, 0.5, 0., 0.45, 0.7, 0.8, 0.45, 0), ncol = 4)
c1 <- hclust(as.dist(m), method = "complete")
plot(c1)
c2 <- hclust(as.dist(m), method = "single")
plot(c2)
table(1:4, cutree(c1, 2))
table(1:4, cutree(c2, 2))
plot(c1, labels = c(2, 1, 3, 4))
library(ggplot2)
d <- data.frame(
x1 = c(1, 1, 0, 5, 6, 4),
x2 = c(4, 3, 4, 1, 2, 0)
)
ggplot(d, aes(x = x1, y = x2)) + geom_point()
set.seed(42)
d$cluster <- sample(c(1, 2), size = nrow(d), replace = TRUE)
d
centroids <- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2]))
centroids
dist <- sapply(1:2, function(i) {
sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2)
})
d$cluster <- apply(dist, 1, which.min)
d
centroids <- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2]))
dist <- sapply(1:2, function(i) {
sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2)
})
d$cluster <- apply(dist, 1, which.min)
d
ggplot(d, aes(x = x1, y = x2, color = factor(cluster))) + geom_point()
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine_data <- read.csv(url, header = FALSE)
colnames(wine_data) <- c("Class", "Alcohol", "Malic_Acid", "Ash", "Alcalinity_of_Ash", "Magnesium", "Total_phenols", "Flavanoids", "Nonflavanoid_phenols", "Proanthocyanins", "Color_intensity", "Hue", "OD280_OD315", "Proline")
head(wine_data)
summary(wine_data)
print("Performing PCA with scaling")
wine_pca <- prcomp(wine_data[, -1], scale = TRUE)
print("Plotting biplot")
biplot(wine_pca, cex=0.5)
cor(wine_data[, "Hue"], wine_data[,"Malic_Acid"])
print("Identifying feature opposite to Hue")
biplot_vectors <- wine_pca$rotation[, 1:2]
hue_vector <- biplot_vectors["Hue",]
opposite_feature <- which.min(colSums(t(biplot_vectors) * hue_vector))
print(opposite_feature)
correlation_value <- cor(wine_data$Hue, wine_data[, opposite_feature + 1])
print(paste("The feature", colnames(wine_data)[opposite_feature + 1], "has a correlation of", correlation_value, "with Hue"))
print("Plotting scree plot")
screeplot(wine_pca)
summary(wine_pca)
variance_explained <- summary(wine_pca)$importance[2, 1:2] * 100
print(paste("Percentage of total variance explained by PC1 and PC2 are", variance_explained[1], "% and", variance_explained[2], "%, respectively."))
data("USArrests")
arrest_data <- na.omit(USArrests)
head(arrest_data)
summary(arrest_data)
print("Data is scaled due to different units and scales of features.")
arrest_data_scaled <- scale(arrest_data)
wss <- numeric(9)
for (k in 2:10) {
kmeans_model <- kmeans(arrest_data_scaled, centers = k, nstart = 25)
wss[k - 1] <- kmeans_model$tot.withinss
}
wss
print("Plotting within-cluster sum of squares for each k")
wss_df <- tibble(K = 2:10, WSS = wss)
ggplot(wss_df, aes(x = K, y = WSS)) +
geom_line() +
geom_point() +
labs(title = "Elbow Method for Choosing k", x = "Number of Clusters (k)", y = "Within-cluster Sum of Squares")
optimal_k <- which(diff(diff(wss)) == min(diff(diff(wss)))) + 2
print(paste("The optimal number of clusters is", optimal_k))
final_kmeans <- kmeans(arrest_data_scaled, centers = optimal_k, nstart = 25)
print(final_kmeans)
print("Plotting optimal clustering")
final_kmeans <- kmeans(arrest_data_scaled, centers = 8, nstart = 25)
fviz_cluster(final_kmeans, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
print("Plotting clustering for k = 4")
final_kmeans_k4 <- kmeans(arrest_data_scaled, centers = 4, nstart = 25)
fviz_cluster(final_kmeans_k4, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine_quality <- read.csv(url, sep = ";")
wine_data <- wine_quality[, -ncol(wine_quality)]
summary(wine_data)
wine_data_scaled <- scale(wine_data)
summary(wine_data_scaled)
hc_single <- hclust(dist(wine_data_scaled), method = "single")
hc_complete <- hclust(dist(wine_data_scaled), method = "complete")
plot(hc_single, main = "Single Linkage", sub = "", xlab = "")
plot(hc_complete, main = "Complete Linkage", sub = "", xlab = "")
distance_single <- hc_single$height[length(hc_single$height)-1]
distance_complete <- hc_complete$height[length(hc_complete$height)-1]
print(paste("Distance at which penultimate clusters merge for single linkage:", distance_single))
print(paste("Distance at which penultimate clusters merge for complete linkage:", distance_complete))
clusters_single <- cutree(hc_single, k = 2)
clusters_complete <- cutree(hc_complete, k = 2)
summary_stats_single <- aggregate(wine_data, by = list(cluster = clusters_single), mean)
summary_stats_complete <- aggregate(wine_data, by = list(cluster = clusters_complete), mean)
summary_stats_single
summary_stats_complete
mean_diffs_single <- sapply(2:ncol(summary_stats_single), function(i) {
abs(summary_stats_single[[i]][1] - summary_stats_single[[i]][2])
})
names(mean_diffs_single) <- colnames(wine_data)
mean_diffs_complete <- sapply(2:ncol(summary_stats_complete), function(i) {
abs(summary_stats_complete[[i]][1] - summary_stats_complete[[i]][2])
})
names(mean_diffs_complete) <- colnames(wine_data)
print("Feature means with the largest differences for Single Linkage")
print(mean_diffs_single)
print("Feature means with the largest differences for Complete Linkage")
print(mean_diffs_complete)
sorted_diffs_single <- sort(mean_diffs_single, decreasing = TRUE)
top_3_single <- sorted_diffs_single[1:3]
print("Feature means with the largest differences for Single Linkage:")
print(top_3_single)
sorted_diffs_complete <- sort(mean_diffs_complete, decreasing = TRUE)
top_3_complete <- sorted_diffs_complete[1:3]
print("Feature means with the largest differences for Complete Linkage:")
print(top_3_complete)
sizes_single <- table(clusters_single)
sizes_complete <- table(clusters_complete)
balance_single <- abs(sizes_single[1] - sizes_single[2])
balance_complete <- abs(sizes_complete[1] - sizes_complete[2])
print(paste("Balance of clustering for single linkage:", balance_single))
print(paste("Balance of clustering for complete linkage:", balance_complete))
sizes_single
sizes_complete
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)
# surpress warnings
options(warn = -1)
# install libraries (if not installed) and load them
packages <- c("utils", "ggplot2", "tidyverse", "factoextra", "cluster")
for (pkg in packages) {
suppressWarnings(
suppressMessages(
if (!require(pkg, character.only = TRUE)) {
install.packages(pkg)
library(pkg, character.only = TRUE)
}
)
)
}
m <- matrix(c(0, 0.3, 0.4, 0.7, 0.3, 0, 0.5, 0.8, 0.4, 0.5, 0., 0.45, 0.7, 0.8, 0.45, 0), ncol = 4)
c1 <- hclust(as.dist(m), method = "complete")
plot(c1)
c2 <- hclust(as.dist(m), method = "single")
plot(c2)
table(1:4, cutree(c1, 2))
table(1:4, cutree(c2, 2))
plot(c1, labels = c(2, 1, 3, 4))
library(ggplot2)
d <- data.frame(
x1 = c(1, 1, 0, 5, 6, 4),
x2 = c(4, 3, 4, 1, 2, 0)
)
ggplot(d, aes(x = x1, y = x2)) + geom_point()
set.seed(42)
d$cluster <- sample(c(1, 2), size = nrow(d), replace = TRUE)
d
centroids <- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2]))
centroids
dist <- sapply(1:2, function(i) {
sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2)
})
d$cluster <- apply(dist, 1, which.min)
d
centroids <- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2]))
dist <- sapply(1:2, function(i) {
sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2)
})
d$cluster <- apply(dist, 1, which.min)
d
ggplot(d, aes(x = x1, y = x2, color = factor(cluster))) + geom_point()
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
wine_data <- read.csv(url, header = FALSE)
colnames(wine_data) <- c("Class", "Alcohol", "Malic_Acid", "Ash", "Alcalinity_of_Ash", "Magnesium", "Total_phenols", "Flavanoids", "Nonflavanoid_phenols", "Proanthocyanins", "Color_intensity", "Hue", "OD280_OD315", "Proline")
head(wine_data)
summary(wine_data)
print("Performing PCA with scaling")
wine_pca <- prcomp(wine_data[, -1], scale = TRUE)
print("Plotting biplot")
biplot(wine_pca, cex=0.5)
cor(wine_data[, "Hue"], wine_data[,"Malic_Acid"])
print("Identifying feature opposite to Hue")
biplot_vectors <- wine_pca$rotation[, 1:2]
hue_vector <- biplot_vectors["Hue",]
opposite_feature <- which.min(colSums(t(biplot_vectors) * hue_vector))
print(opposite_feature)
correlation_value <- cor(wine_data$Hue, wine_data[, opposite_feature + 1])
print(paste("The feature", colnames(wine_data)[opposite_feature + 1], "has a correlation of", correlation_value, "with Hue"))
print("Plotting scree plot")
screeplot(wine_pca)
summary(wine_pca)
variance_explained <- summary(wine_pca)$importance[2, 1:2] * 100
print(paste("Percentage of total variance explained by PC1 and PC2 are", variance_explained[1], "% and", variance_explained[2], "%, respectively."))
data("USArrests")
arrest_data <- na.omit(USArrests)
head(arrest_data)
summary(arrest_data)
print("Data is scaled due to different units and scales of features.")
arrest_data_scaled <- scale(arrest_data)
wss <- numeric(9)
for (k in 2:10) {
kmeans_model <- kmeans(arrest_data_scaled, centers = k, nstart = 25)
wss[k - 1] <- kmeans_model$tot.withinss
}
wss
print("Plotting within-cluster sum of squares for each k")
wss_df <- tibble(K = 2:10, WSS = wss)
ggplot(wss_df, aes(x = K, y = WSS)) +
geom_line() +
geom_point() +
labs(title = "Elbow Method for Choosing k", x = "Number of Clusters (k)", y = "Within-cluster Sum of Squares")
optimal_k <- which(diff(diff(wss)) == min(diff(diff(wss)))) + 2
print(paste("The optimal number of clusters is", optimal_k))
final_kmeans <- kmeans(arrest_data_scaled, centers = optimal_k, nstart = 25)
print(final_kmeans)
print("Plotting optimal clustering")
final_kmeans <- kmeans(arrest_data_scaled, centers = 8, nstart = 25)
fviz_cluster(final_kmeans, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
print("Plotting clustering for k = 4")
final_kmeans_k4 <- kmeans(arrest_data_scaled, centers = 4, nstart = 25)
fviz_cluster(final_kmeans_k4, data = arrest_data_scaled, geom = "point", stand = FALSE, ellipse = TRUE, ggtheme = theme_minimal())
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
wine_quality <- read.csv(url, sep = ";")
wine_data <- wine_quality[, -ncol(wine_quality)]
summary(wine_data)
wine_data_scaled <- scale(wine_data)
summary(wine_data_scaled)
hc_single <- hclust(dist(wine_data_scaled), method = "single")
hc_complete <- hclust(dist(wine_data_scaled), method = "complete")
plot(hc_single, main = "Single Linkage", sub = "", xlab = "")
plot(hc_complete, main = "Complete Linkage", sub = "", xlab = "")
distance_single <- hc_single$height[length(hc_single$height)-1]
distance_complete <- hc_complete$height[length(hc_complete$height)-1]
print(paste("Distance at which penultimate clusters merge for single linkage:", distance_single))
print(paste("Distance at which penultimate clusters merge for complete linkage:", distance_complete))
clusters_single <- cutree(hc_single, k = 2)
clusters_complete <- cutree(hc_complete, k = 2)
summary_stats_single <- aggregate(wine_data, by = list(cluster = clusters_single), mean)
summary_stats_complete <- aggregate(wine_data, by = list(cluster = clusters_complete), mean)
summary_stats_single
summary_stats_complete
mean_diffs_single <- sapply(2:ncol(summary_stats_single), function(i) {
abs(summary_stats_single[[i]][1] - summary_stats_single[[i]][2])
})
names(mean_diffs_single) <- colnames(wine_data)
mean_diffs_complete <- sapply(2:ncol(summary_stats_complete), function(i) {
abs(summary_stats_complete[[i]][1] - summary_stats_complete[[i]][2])
})
names(mean_diffs_complete) <- colnames(wine_data)
print("Feature means with the largest differences for Single Linkage")
print(mean_diffs_single)
print("Feature means with the largest differences for Complete Linkage")
print(mean_diffs_complete)
sorted_diffs_single <- sort(mean_diffs_single, decreasing = TRUE)
top_3_single <- sorted_diffs_single[1:3]
print("Feature means with the largest differences for Single Linkage:")
print(top_3_single)
sorted_diffs_complete <- sort(mean_diffs_complete, decreasing = TRUE)
top_3_complete <- sorted_diffs_complete[1:3]
print("Feature means with the largest differences for Complete Linkage:")
print(top_3_complete)
sizes_single <- table(clusters_single)
sizes_complete <- table(clusters_complete)
balance_single <- abs(sizes_single[1] - sizes_single[2])
balance_complete <- abs(sizes_complete[1] - sizes_complete[2])
print(paste("Balance of clustering for single linkage:", balance_single))
print(paste("Balance of clustering for complete linkage:", balance_complete))
sizes_single
sizes_complete
