---
title: "R Notebook"
output: html_notebook
---


# Clean up


```{r}
library(dplyr)
library(tidyr)
library(caret)
library(corrplot)

data <- read.csv("dataset/master_rank.csv")
```

```{r}
head(data)
```

```{r}
print(length(colnames(data)))
print(nrow(data))
```

```{r}
# Step 1: Data Cleaning
# Handling missing values
data <- data %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%
  mutate(across(where(is.character), ~ifelse(is.na(.), "Unknown", .)))
```



```{r}
print(length(colnames(data)))
print(nrow(data))
total_NAs <- sum(is.na(data))
print(paste("Total NA values in the dataset:", total_NAs))
```

```{r}
colnames(data)
```

```{r}
#columns_to_scale <- c("Age.x", "G.x", "PA", "AB", "R", "H", "X2B.x", "X3B.x", "HR", "RBI", "SB", "CS", "BB", "SO", "BA", "OBP", "SLG", "OPS", "OPS.", "TB", "GDP", "HBP", "SF", "IBB", "Wt")
#data[columns_to_scale] <- scale(data[columns_to_scale])
```



```{r}
# Step 4: Checking for high correlations
cor_data <- cor(data[, sapply(data, is.numeric)], use = "complete.obs")
high_cor_pairs <- which(cor_data > 0.70 & upper.tri(cor_data), arr.ind = TRUE)

# Create a table of highly correlated pairs
if (length(high_cor_pairs) > 0) {
  high_cor_data <- data.frame(
    Feature1 = names(data)[high_cor_pairs[, 1]],
    Feature2 = names(data)[high_cor_pairs[, 2]],
    Correlation = cor_data[high_cor_pairs]
  )
  print("Highly correlated pairs (above 0.86):")
  print(high_cor_data)
} else {
  print("No highly correlated pairs found.")
}
```


```{r}
# Function to drop columns
drop_columns <- function(data, columns_to_drop) {
  data[, !names(data) %in% columns_to_drop]
}

columns_to_drop <- c("Player","Team","Year", "Age.x","CS", "OPS.", "GDP", "HBP", "SH", "SF", "source.x", "Age.y", "X", "B", "T", "Ht", "Wt", "DoB", "Yrs", "G.y","P",  "C", "X1B", "X2B.y", "X3B.y", "SS", "LF", "CF", "RF", "OF", "DH", "PH", "PR", "source.y", "X.1", 'Pos')

# Drop the specified columns from the dataset
data <- drop_columns(data, columns_to_drop)


```

```{r}
colnames(data)
```


```{r}
# Rename the columns using the `names()` function
names(data) <- c(
  "Rank",                # "Rk" 
  "GamesPlayed",         # "G.x"
  "PlateAppearances",    # "PA"
  "AtBats",              # "AB"
  "RunsScored",          # "R"
  "Hits",                # "H"
  "Doubles",             # "X2B.x"
  "Triples",             # "X3B.x"
  "HomeRuns",            # "HR"
  "RunsBattedIn",        # "RBI"
  "StolenBases",         # "SB"
  "BaseOnBalls",         # "BB"
  "Strikeouts",          # "SO"
  "BattingAverage",      # "BA"
  "OnBasePercentage",    # "OBP"
  "SluggingPercentage",  # "SLG"
  "OnBasePlusSlugging",  # "OPS"
  "TotalBases",          # "TB"
  "IntentionalWalks",    # "IBB"
  "GamesStarted",        # "GS"
  "BattingStats",        # "Batting" (Assumed)
  "DefenseStats",        # "Defense" (Assumed)
  "WinsAboveReplacement",# "WAR"
  "Salary",              # "Salary"
  "Top100Status"         # "top_100"
)

# Clarify or investigate what 'UnknownX1' represents
# If 'UnknownX1' is still unidentified or not useful, consider dropping it as well

columns_to_drop <- c()  # Update based on new info

# Drop the specified columns from the dataset using previously defined function
data <- drop_columns(data, columns_to_drop)


print(length(colnames(data)))
colnames(data)
```


```{r}

# Load necessary library
library(dplyr)

# Calculate correlations
cor_data <- cor(data[, sapply(data, is.numeric)], use = "complete.obs")
high_cor_pairs <- which(cor_data > 0.92 & upper.tri(cor_data), arr.ind = TRUE)

# Create and sort a table of highly correlated pairs
if (length(high_cor_pairs) > 0) {
  high_cor_data <- data.frame(
    Feature1 = names(data)[high_cor_pairs[, 1]],
    Feature2 = names(data)[high_cor_pairs[, 2]],
    Correlation = cor_data[high_cor_pairs]
  )

  # Group by Feature1 and count the number of rows for each Feature1
  feature1_counts <- high_cor_data %>%
    group_by(Feature1) %>%
    summarize(Count = n(), .groups = 'drop') %>% # `n()` counts the number of rows in each group
    arrange(desc(Count))  # Sorting by Count in descending order

  print("Count of high correlations per feature (Feature1):")
  print(feature1_counts)
} else {
  print("No highly correlated pairs found.")
}


```


```{r}
columns_to_drop <- c("PlateAppearances","AtBats", "GamesPlayed","RunsScored","Hits","Doubles","RunsBattedIn","TotalBases")
data <- drop_columns(data, columns_to_drop)
```


```{r}
cor_data <- cor(data[, sapply(data, is.numeric)], use = "complete.obs")
high_cor_pairs <- which(cor_data > 0.92 & upper.tri(cor_data), arr.ind = TRUE)

# Create and sort a table of highly correlated pairs
if (length(high_cor_pairs) > 0) {
  high_cor_data <- data.frame(
    Feature1 = names(data)[high_cor_pairs[, 1]],
    Feature2 = names(data)[high_cor_pairs[, 2]],
    Correlation = cor_data[high_cor_pairs]
  )

  # Group by Feature1 and count the number of rows for each Feature1
  feature1_counts <- high_cor_data %>%
    group_by(Feature1) %>%
    summarize(Count = n(), .groups = 'drop') %>% # `n()` counts the number of rows in each group
    arrange(desc(Count))  # Sorting by Count in descending order

  print("Count of high correlations per feature (Feature1):")
  print(feature1_counts)
} else {
  print("No highly correlated pairs found.")
}

```

```{r}
colnames(data)
```

# Heatmap

```{r}
corrplot(cor_data, method = "color", order = "hclust", tl.cex = 0.8,
         title = "Heatmap of Correlation Matrix", 
         tl.col = "black", tl.srt = 90)

```
As we can see, WAR isn't highly correlated to one feature.

# PCA

```{r}
pca_model <- prcomp(data[, sapply(data, is.numeric)], center = TRUE, scale. = TRUE)
summary(pca_model)
```

 - The first principal component captures 35.92% of the variance within the dataset, highlighting its significant role in encompassing the key features and variations of the data.
 - By retaining the first 10 principal components, which cumulatively account for over 81% of the dataset's variance, we can effectively reduce dimensionality while preserving the majority of the information.
 - The rapid decline in the proportion of variance explained by subsequent components beyond the 15th (reaching nearly 97.23% cumulative variance), suggests that further components contribute minimally to the data's overall structure.
 - These results support using the top 10 to 15 components for further data analysis, simplifying the computational demands and potentially enhancing the clarity of patterns and trends in the data.
 

 
```{r}
plot(pca_model, type = "l")
```

# Step 2: Exploratory Data Analysis (EDA)

## Descriptive Statistics:




```{r}
# Histogram for Wins Above Replacement (WAR)
ggplot(data, aes(x = WinsAboveReplacement)) + 
  geom_histogram(bins = 30, fill = "red", color = "black") +
  theme_minimal() +
  labs(title = "Histogram of Wins Above Replacement (WAR)", x = "WAR")

```



### Compute 5-Number Summaries

```{r}
# Selecting only continuous variables for summary
continuous_vars <- data[, sapply(data, is.numeric)]
summary(continuous_vars)

```

### Findings:
- **Performance Variability**: There is considerable variability in player performance across different metrics, with a few outliers significantly outperforming others.
- **Skewness in Distributions**: Many of the metrics show right-skewed distributions, where the majority of players are below the average, pulled up by high-performing outliers.
- **Physical and Role Diversity**: The wide range in physical stats (like Weight) and role-specific stats (like Games Started or Pitching) reflects the diversity in player roles and physical builds, emphasizing the varied nature of professional baseball.
- **Impact Players**: The range in Wins Above Replacement highlights the presence of both underperformers and impact players within the league, showcasing the potential for significant individual contributions to team success.


```{r}
write.csv(data, "dataset/processed_data.csv", row.names = FALSE)
```





